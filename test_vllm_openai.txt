# 1. First install required packages
!pip install vllm openai requests

# 2. Start vLLM server in a background process
import subprocess
import time
import requests

MODEL = "facebook/opt-125m"  # Small CPU-friendly model
PORT = 8000

# Start the server
server_process = subprocess.Popen(
    f"vllm serve {MODEL} --device cpu --port {PORT}",
    shell=True,
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE,
    text=True
)

# Check if server is ready
def is_server_ready():
    try:
        response = requests.get(f"http://localhost:{PORT}/v1/models", timeout=2)
        return response.status_code == 200
    except:
        return False

print("Waiting for server to start...")
for _ in range(30):  # 30 second timeout
    if is_server_ready():
        print("Server ready!")
        break
    time.sleep(1)
else:
    server_process.terminate()
    raise RuntimeError("Failed to start vLLM server")

# 3. Test the OpenAI-compatible endpoint
import openai

client = openai.OpenAI(
    base_url=f"http://localhost:{PORT}/v1",
    api_key="no-key-needed"
)

# Test completion
prompt = "Explain machine learning in simple terms:"
response = client.completions.create(
    model=MODEL,
    prompt=prompt,
    max_tokens=100,
    temperature=0.7
)

print("Prompt:", prompt)
print("Response:", response.choices[0].text)

# 4. Test chat-style completion
conversation = [
    {"role": "user", "content": "What is AI?"},
    {"role": "assistant", "content": "AI stands for Artificial Intelligence."},
    {"role": "user", "content": "How is it different from ML?"}
]

# Format as text prompt
prompt_text = "\n".join([f"{m['role']}: {m['content']}" for m in conversation])
prompt_text += "\nassistant:"

response = client.completions.create(
    model=MODEL,
    prompt=prompt_text,
    max_tokens=100,
    temperature=0.7,
    stop=["user:"]
)

print("\nChat response:", response.choices[0].text)

# 5. Cleanup when done
server_process.terminate()



===============================
# 1. Install required packages (uncomment if needed)
# !pip install vllm openai requests

# 2. Start vLLM server with proper waiting
import subprocess
import time
import requests
from IPython.display import display, Markdown, clear_output

MODEL = "facebook/opt-125m"  # Small CPU-friendly model
PORT = 8000

def start_vllm_server():
    """Start vLLM server and wait until ready"""
    display(Markdown("**Starting vLLM server...**"))
    
    # Start the server process
    server_process = subprocess.Popen(
        f"vllm serve {MODEL} --device cpu --port {PORT}",
        shell=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    
    # Function to check server status
    def is_server_ready():
        try:
            return requests.get(f"http://localhost:{PORT}/v1/models", timeout=2).status_code == 200
        except:
            return False
    
    # Wait for server with progress indication
    start_time = time.time()
    timeout = 120  # 2 minute timeout (might need longer for first-time model download)
    
    while (time.time() - start_time) < timeout:
        if is_server_ready():
            display(Markdown(f"**âœ“ Server ready after {int(time.time()-start_time)} seconds!**"))
            return server_process
        
        # Show animated waiting indicator
        for i in range(5):
            clear_output(wait=True)
            display(Markdown(f"**Waiting for server to start...** {'>'*i}"))
            time.sleep(0.5)
    
    # If we get here, server didn't start
    server_process.terminate()
    raise RuntimeError(f"Server failed to start within {timeout} seconds")

# Start the server (this will block until ready or timeout)
server_process = start_vllm_server()

# 3. Test OpenAI-compatible endpoint
import openai

display(Markdown("**Testing OpenAI-compatible endpoint...**"))

# Initialize client
client = openai.OpenAI(
    base_url=f"http://localhost:{PORT}/v1",
    api_key="no-key-needed"
)

# Simple completion test
def test_completion(prompt, max_tokens=50, temperature=0.7):
    try:
        start_time = time.time()
        
        response = client.completions.create(
            model=MODEL,
            prompt=prompt,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        elapsed = time.time() - start_time
        display(Markdown(f"""
**Prompt:**  
`{prompt}`

**Response (took {elapsed:.2f}s):**  
{response.choices[0].text}

---
        """))
        return response
    except Exception as e:
        display(Markdown(f"**Error:** {str(e)}"))
        return None

# Run tests
test_completion("Explain machine learning in simple terms:")
test_completion("Translate to French: 'Good morning, how are you?'")

# 4. Test chat-style completion
def test_chat(conversation, max_tokens=100, temperature=0.7):
    try:
        # Format conversation history
        prompt_text = "\n".join([f"{m['role']}: {m['content']}" for m in conversation])
        prompt_text += "\nassistant:"
        
        start_time = time.time()
        response = client.completions.create(
            model=MODEL,
            prompt=prompt_text,
            max_tokens=max_tokens,
            temperature=temperature,
            stop=["user:"]
        )
        
        elapsed = time.time() - start_time
        display(Markdown(f"""
**Conversation History:**  
