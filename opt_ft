import os
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import GPT2Tokenizer  # Use GPT2 tokenizer for OPT models
from torch.optim import AdamW

# Step 1: Define the model architecture (OPT-125M)
class OPT125M(torch.nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers, num_heads):
        super(OPT125M, self).__init__()
        self.embedding = torch.nn.Embedding(vocab_size, hidden_size)
        self.transformer_blocks = torch.nn.ModuleList([
            torch.nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads)
            for _ in range(num_layers)
        ])
        self.fc_out = torch.nn.Linear(hidden_size, vocab_size)

    def forward(self, input_ids):
        x = self.embedding(input_ids)
        for block in self.transformer_blocks:
            x = block(x)
        logits = self.fc_out(x)
        return logits

# Step 2: Load the tokenizer (GPT2 tokenizer is compatible with OPT)
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # Set pad token

# Step 3: Load the OPT-125M model from local
def load_opt_125m_local(model_path):
    # Load the model state dict
    state_dict = torch.load(os.path.join(model_path, "pytorch_model.bin"))
    
    # Initialize the model
    model = OPT125M(
        vocab_size=50272,  # Vocabulary size for OPT-125M
        hidden_size=768,   # Hidden size for OPT-125M
        num_layers=12,     # Number of layers for OPT-125M
        num_heads=12       # Number of attention heads for OPT-125M
    )
    
    # Load the state dict into the model
    model.load_state_dict(state_dict)
    return model

# Replace with your local path to the OPT-125M model
model_path = "./opt-125m-local"
model = load_opt_125m_local(model_path)

# Step 4: Prepare a custom dataset
class CustomDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=128):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )
        return encoding["input_ids"].squeeze(0)  # Remove batch dimension

# Example dataset
texts = [
    "This is the first text.",
    "This is the second text.",
    "Fine-tuning the OPT-125M model."
]
dataset = CustomDataset(texts, tokenizer)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True)

# Step 5: Define the optimizer and loss function
optimizer = AdamW(model.parameters(), lr=5e-5)
loss_fn = torch.nn.CrossEntropyLoss()

# Step 6: Fine-tune the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.train()

num_epochs = 3
for epoch in range(num_epochs):
    for batch in dataloader:
        # Move batch to device
        batch = batch.to(device)
        
        # Forward pass
        logits = model(batch)
        
        # Compute loss (shift logits and labels for language modeling)
        shift_logits = logits[:, :-1, :].contiguous()
        shift_labels = batch[:, 1:].contiguous()
        loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        print(f"Epoch {epoch + 1}, Loss: {loss.item()}")

# Step 7: Save the fine-tuned model locally
output_dir = "./fine-tuned-opt-125m"
os.makedirs(output_dir, exist_ok=True)

# Save the model state dict
torch.save(model.state_dict(), os.path.join(output_dir, "pytorch_model.bin"))

# Save the tokenizer
tokenizer.save_pretrained(output_dir)

# Step 8: Load the fine-tuned model (optional)
def load_fine_tuned_model(model_path):
    model = OPT125M(
        vocab_size=50272,
        hidden_size=768,
        num_layers=12,
        num_heads=12
    )
    state_dict = torch.load(os.path.join(model_path, "pytorch_model.bin"))
    model.load_state_dict(state_dict)
    return model

fine_tuned_model = load_fine_tuned_model(output_dir)
fine_tuned_model.to(device)

# Step 9: Test the fine-tuned model
input_text = "Once upon a time"
input_ids = tokenizer(input_text, return_tensors="pt")["input_ids"].to(device)
with torch.no_grad():
    outputs = fine_tuned_model(input_ids)
    predicted_token_ids = torch.argmax(outputs, dim=-1)
    predicted_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)
    print("Generated Text:", predicted_text)
