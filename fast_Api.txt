from fastapi import FastAPI
from pydantic import BaseModel
from vllm import LLM, SamplingParams

# Initialize FastAPI app
app = FastAPI()

# Load the model using vLLM
llm = LLM(model="facebook/opt-125m")

# Define request schema
class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 100

# Define response route
@app.post("/generate")
async def generate_text(request: GenerateRequest):
    sampling_params = SamplingParams(max_tokens=request.max_tokens)
    output = llm.generate(request.prompt, sampling_params)
    return {"response": output[0].outputs[0].text}

# Health check route
@app.get("/")
async def root():
    return {"message": "vLLM FastAPI server is running"}


bbbbbbbb

import streamlit as st
import subprocess
import threading
import time
import openai
import requests
import sys
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.entrypoints.openai import api_server

# Configuration
MODEL_NAME = "facebook/opt-125m"  # Small model suitable for CPU
API_PORT = 8000

# --- vLLM Server in Subprocess ---
def run_vllm_server():
    # Create a minimal FastAPI app
    app = FastAPI()
    
    # Configure CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # Initialize engine with CPU compatibility
    engine_args = AsyncEngineArgs(
        model=MODEL_NAME,
        tensor_parallel_size=1,
        device="cpu",
        dtype="float32",
    )
    
    engine = AsyncLLMEngine.from_engine_args(engine_args)
    app = api_server.create_app(engine=engine)
    
    # Use the built-in ASGI server from vLLM
    from vllm.entrypoints.openai.serving import serve_application
    serve_application(app, host="0.0.0.0", port=API_PORT)

# --- Streamlit Interface ---
def streamlit_interface():
    # Configure OpenAI client
    client = openai.OpenAI(
        base_url=f"http://localhost:{API_PORT}/v1",
        api_key="no-key-needed"
    )
    
    st.title("vLLM Chat Interface (CPU)")
    st.write(f"Using model: {MODEL_NAME}")
    
    # Model parameters
    with st.sidebar:
        st.header("Model Parameters")
        temperature = st.slider("Temperature", 0.1, 1.0, 0.7)
        max_tokens = st.slider("Max Tokens", 10, 500, 100)
    
    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Accept user input
    if prompt := st.chat_input("What would you like to ask?"):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Display assistant response
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            try:
                # Format messages for completions API
                prompt_text = "\n".join([f"{m['role']}: {m['content']}" for m in st.session_state.messages])
                prompt_text += "\nassistant:"
                
                # Call vLLM server using completions API
                response = client.completions.create(
                    model=MODEL_NAME,
                    prompt=prompt_text,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    stop=["user:"],  # Stop generation if we see another user prompt
                    stream=False,
                )
                
                full_response = response.choices[0].text.strip()
                message_placeholder.markdown(full_response)
                
            except Exception as e:
                st.error(f"Error calling the model: {str(e)}")
                full_response = "Sorry, I encountered an error."
                message_placeholder.markdown(full_response)
        
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": full_response})

# --- Health Check ---
def check_server_ready():
    url = f"http://localhost:{API_PORT}/v1/models"
    for _ in range(10):  # Try for 10 seconds
        try:
            response = requests.get(url)
            if response.status_code == 200:
                return True
        except:
            pass
        time.sleep(1)
    return False

# --- Main Execution ---
if __name__ == "__main__":
    # Start vLLM server in a subprocess
    server_process = subprocess.Popen(
        [sys.executable, "-c", 
         f"from {__name__} import run_vllm_server; run_vllm_server()"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE
    )
    
    # Wait for server to be ready
    if not check_server_ready():
        st.error("Failed to start vLLM server")
        server_process.terminate()
        st.stop()
    
    # Run Streamlit interface
    streamlit_interface()
    
    # Cleanup when Streamlit is done
    server_process.terminate()










=============================================

import streamlit as st
import subprocess
import time
import requests
import openai
import sys
import os
from threading import Thread

# Configuration
MODEL_NAME = "facebook/opt-125m"  # Small CPU-friendly model
PORT = 8000

# --- vLLM Server Management ---
def start_vllm_server():
    cmd = f"vllm serve {MODEL_NAME} --device cpu --port {PORT}"
    process = subprocess.Popen(
        cmd,
        shell=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    
    # Log server output in background
    def log_output(pipe, prefix):
        for line in pipe:
            print(f"[{prefix}] {line}", end='')
    
    Thread(target=log_output, args=(process.stdout, "stdout"), daemon=True).start()
    Thread(target=log_output, args=(process.stderr, "stderr"), daemon=True).start()
    
    return process

def is_server_ready():
    try:
        response = requests.get(f"http://localhost:{PORT}/v1/models", timeout=2)
        return response.status_code == 200
    except:
        return False

# --- Streamlit UI ---
def chat_interface():
    st.title("vLLM Chat (CPU Mode)")
    st.caption(f"Model: {MODEL_NAME} | Running on CPU")
    
    # Initialize client
    client = openai.OpenAI(
        base_url=f"http://localhost:{PORT}/v1",
        api_key="no-key-needed"
    )
    
    # Session state for messages
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    # Display messages
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])
    
    # Sidebar controls
    with st.sidebar:
        st.subheader("Generation Parameters")
        temperature = st.slider("Temperature", 0.1, 1.0, 0.7)
        max_tokens = st.slider("Max Tokens", 10, 200, 100)
    
    # Chat input
    if prompt := st.chat_input("Your message"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)
        
        with st.chat_message("assistant"):
            response_placeholder = st.empty()
            full_response = ""
            
            try:
                # Format conversation history
                conversation = "\n".join(
                    f"{m['role']}: {m['content']}" 
                    for m in st.session_state.messages
                )
                prompt_text = f"{conversation}\nassistant:"
                
                # Get completion
                response = client.completions.create(
                    model=MODEL_NAME,
                    prompt=prompt_text,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    stop=["user:"],
                    stream=True
                )
                
                # Stream response
                for chunk in response:
                    full_response += chunk.choices[0].text
                    response_placeholder.markdown(full_response + "â–Œ")
                
                response_placeholder.markdown(full_response)
                
            except Exception as e:
                st.error(f"Error: {str(e)}")
                full_response = "Sorry, I encountered an error"
                response_placeholder.markdown(full_response)
        
        st.session_state.messages.append({"role": "assistant", "content": full_response})

# --- Main Execution ---
if __name__ == "__main__":
    # Start server
    st.info("Starting vLLM server...")
    server_process = start_vllm_server()
    
    # Wait for server
    with st.spinner("Waiting for server to start..."):
        for _ in range(30):  # 30 second timeout
            if is_server_ready():
                break
            time.sleep(1)
        else:
            st.error("Failed to start vLLM server")
            server_process.terminate()
            st.stop()
    
    # Run chat interface
    st.success("Server ready!")
    chat_interface()
    
    # Cleanup on exit
    def cleanup():
        server_process.terminate()
        server_process.wait()
    
    import atexit
    atexit.register(cleanup)
