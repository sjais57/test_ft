import PyPDF2

def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text()
    return text

# Example usage
pdf_path = "your_pdf_file.pdf"
text = extract_text_from_pdf(pdf_path)
print("Extracted text from PDF.")


import nltk
from nltk.tokenize import sent_tokenize

# Download NLTK data (only needed once)
nltk.download('punkt')

def preprocess_text(text):
    # Tokenize into sentences
    sentences = sent_tokenize(text)
    
    # Remove short sentences (optional)
    sentences = [s for s in sentences if len(s.split()) > 5]
    
    # Join sentences into paragraphs
    processed_text = "\n\n".join(sentences)
    return processed_text

# Preprocess the extracted text
processed_text = preprocess_text(text)
print("Preprocessed text.")



import json

def create_qa_dataset(text, output_file):
    # Split the text into chunks (e.g., paragraphs or sections)
    chunks = text.split("\n\n")  # Adjust the splitting logic as needed
    
    # Create a list of QA pairs
    qa_pairs = []
    for chunk in chunks:
        if chunk.strip():  # Skip empty chunks
            # Example: Generate a question and answer
            question = f"What is the main idea of this section: '{chunk[:50]}...'?"
            answer = chunk.split(". ")[0]  # Use the first sentence as the answer
            
            # Remove any extra newlines in the context, question, and answer
            context = chunk.replace("\n", " ").strip()
            question = question.replace("\n", " ").strip()
            answer = answer.replace("\n", " ").strip()
            
            qa_pairs.append({
                "context": context,
                "question": question,
                "answer": answer
            })
    
    # Write to a JSONL file
    with open(output_file, "w") as f:
        for item in qa_pairs:
            f.write(json.dumps(item) + "\n")

# Example usage
output_file = "qa_dataset.jsonl"
create_qa_dataset(text, output_file)
print("Created QA dataset in JSONL format.")

#verify dataset:
# Inspect the first few lines of the dataset
with open("qa_dataset.jsonl", "r") as f:
    for i, line in enumerate(f):
        if i < 5:  # Print the first 5 lines
            print(json.loads(line))
        else:
            break


from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset

# Load the tokenizer and model
model_name = "codellama/CodeLlama-7b-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Set a pad_token if it doesn't exist
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as PAD token

model = AutoModelForCausalLM.from_pretrained(model_name)

# Load the QA dataset
dataset = load_dataset('json', data_files={'train': 'qa_dataset.jsonl'})

# Tokenize the dataset with padding
def tokenize_function(examples):
    # Combine context, question, and answer into a single input
    inputs = [f"Context: {c}\nQuestion: {q}\nAnswer: {a}" for c, q, a in zip(examples['context'], examples['question'], examples['answer'])]
    return tokenizer(inputs, padding="max_length", truncation=True, max_length=512, return_tensors="pt")

tokenized_datasets = dataset.map(tokenize_function, batched=True)

# Add labels for causal language modeling
tokenized_datasets = tokenized_datasets.map(lambda x: {"labels": x["input_ids"]}, batched=True)

# Data collator for causal language modeling
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # Set to False for causal language modeling
)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    overwrite_output_dir=True,
    num_train_epochs=3,  # Increase the number of epochs
    per_device_train_batch_size=1,
    save_steps=10_000,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=200,
    do_train=True,
    no_cuda=True,  # Force CPU usage
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets['train'],
    data_collator=data_collator,
)

# Fine-tune the model
print("Starting fine-tuning...")
trainer.train()
print("Fine-tuning complete.")


trainer.save_model("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")
print("Model saved to './fine_tuned_model'.")



from transformers import pipeline

# Load the fine-tuned model
qa_pipeline = pipeline("text-generation", model="./fine_tuned_model", tokenizer=tokenizer)

# Test the model
context = "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France."
question = "Where is the Eiffel Tower located?"

# Generate an answer
input_text = f"Context: {context}\nQuestion: {question}\nAnswer:"
output = qa_pipeline(input_text, max_length=100, num_return_sequences=1)
print("Generated Answer:", output[0]['generated_text'])
