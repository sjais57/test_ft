import openai

openai.api_base = "http://localhost:8000/v1"  # vLLM's default OpenAI-compatible endpoint
openai.api_key = "EMPTY"

response = openai.ChatCompletion.create(
    model="opt-125m",  # Change to "codellama-7b-instruct-hf" for the other model
    messages=[{"role": "system", "content": "You are a helpful assistant."},
              {"role": "user", "content": "What is AI?"}]
)

print(response)

import asyncio

async def get_response(model_name, prompt):
    response = openai.ChatCompletion.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}]
    )
    return response

async def main():
    models = ["opt-125m", "codellama-7b-instruct-hf"]
    tasks = [get_response(model, "Explain machine learning.") for model in models]
    responses = await asyncio.gather(*tasks)
    for model, res in zip(models, responses):
        print(f"Model: {model}, Response: {res}")

asyncio.run(main())



import openai

openai.api_base = "http://localhost:8000/v1"
openai.api_key = "EMPTY"

response = openai.ChatCompletion.create(
    model="opt-125m",
    messages=[{"role": "user", "content": "Explain AI"}]
)

print(response)


======================================

from openai import OpenAI

# Point the client to your local vLLM server
client = OpenAI(
    base_url="http://localhost:8000/v1",  # vLLM's OpenAI-compatible API endpoint
    api_key="token-abc123"  # API key is required but can be arbitrary for vLLM
)

# Define your question
question = "Explain the concept of gravitational waves in simple terms."

# Generate a response
response = client.completions.create(
    model="facebook/opt-1.3b",  # The model you're using
    prompt=f"Question: {question}\nAnswer:",
    max_tokens=150,
    temperature=0.7,
    stop=["\n\n"]  # Stop generation at double newlines
)

# Print the answer
print("Question:", question)
print("Answer:", response.choices[0].text.strip())



from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="token-abc123")

questions = [
    "What is the capital of France?",
    "Explain quantum computing in simple terms.",
    "Who wrote 'To Kill a Mockingbird'?",
    "How does photosynthesis work?"
]

for question in questions:
    response = client.completions.create(
        model="facebook/opt-1.3b",
        prompt=f"Question: {question}\nAnswer:",
        max_tokens=100,
        temperature=0.3
    )
    print(f"Q: {question}")
    print(f"A: {response.choices[0].text.strip()}\n")


try:
    response = client.completions.create(
        model="facebook/opt-1.3b",
        prompt=prompt,
        max_tokens=100
    )
except Exception as e:
    print(f"Error processing question: {question}")
    print(f"Error: {str(e)}")
    continue
==================================

Chat completion:
response = client.chat.completions.create(
    model="facebook/opt-1.3b",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": question}
    ],
    max_tokens=150,
    temperature=0.7
)

print("Answer:", response.choices[0].message.content)



=========================
Conversation:
conversation = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Who invented the telephone?"},
    {"role": "assistant", "content": "Alexander Graham Bell is credited with inventing the telephone."},
    {"role": "user", "content": "What year was it invented?"}
]

response = client.chat.completions.create(
    model="facebook/opt-1.3b",
    messages=conversation,
    max_tokens=50
)

print(response.choices[0].message.content)

============================
Batch processing high through put:
batch_prompts = [f"Question: {q}\nAnswer:" for q in questions]

responses = client.completions.create(
    model="facebook/opt-1.3b",
    prompt=batch_prompts,
    max_tokens=100,
    temperature=0.3
)

for q, resp in zip(questions, responses.choices):
    print(f"Q: {q}")
    print(f"A: {resp.text.strip()}\n")
