import streamlit as st
import openai
import subprocess
import time
import requests
import sys

# Configuration
MODEL = "facebook/opt-125m"  # Small CPU-friendly model
PORT = 8001

# --- Core Functions ---
def start_server():
    """Start vLLM server process"""
    cmd = f"vllm serve {MODEL} --device cpu --port {PORT}"
    process = subprocess.Popen(
        cmd,
        shell=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )
    return process

def check_server_ready():
    """Check if server is ready"""
    try:
        return requests.get(f"http://localhost:{PORT}/v1/models", timeout=2).status_code == 200
    except:
        return False

def init_client():
    """Initialize OpenAI client"""
    return openai.OpenAI(
        base_url=f"http://localhost:{PORT}/v1",
        api_key="no-key-needed"
    )

def get_response(client, messages, max_tokens=100, temperature=0.7):
    """Get chat completion from vLLM"""
    prompt = "\n".join(f"{m['role']}: {m['content']}" for m in messages) + "\nassistant:"
    response = client.completions.create(
        model=MODEL,
        prompt=prompt,
        max_tokens=max_tokens,
        temperature=temperature,
        stop=["user:"]
    )
    return response.choices[0].text.strip()

# --- Streamlit App ---
st.title("vLLM Chat Interface")
st.caption(f"Model: {MODEL} | Port: {PORT}")

# Initialize session state
if 'messages' not in st.session_state:
    st.session_state.messages = []

# Automatically start server and initialize client
if 'client' not in st.session_state:
    with st.spinner("ðŸ”ƒ Starting vLLM server (this may take a minute)..."):
        server_process = start_server()
        
        # Wait for server to start (max 2 minutes)
        start_time = time.time()
        while (time.time() - start_time) < 120:
            if check_server_ready():
                st.session_state.client = init_client()
                st.success("âœ… Server is ready! You can start chatting.")
                break
            time.sleep(1)
        else:
            st.error("âŒ Server failed to start within 2 minutes")
            server_process.terminate()
            st.stop()

# Display chat history
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# Chat input
if prompt := st.chat_input("Type your message..."):
    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    
    # Get and display response
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            try:
                response = get_response(
                    st.session_state.client,
                    st.session_state.messages,
                    max_tokens=100,
                    temperature=0.7
                )
                st.write(response)
                st.session_state.messages.append({"role": "assistant", "content": response})
            except Exception as e:
                st.error(f"Error: {str(e)}")


Goals of Model Evaluation
Assess Model Performance

Measure how well the model predicts outcomes using metrics like accuracy, precision, recall, F1-score, etc.

Check for Overfitting & Underfitting

Determine if the model is too complex (overfitting) or too simple (underfitting).

Compare Different Models

Evaluate multiple models to select the best-performing one.

Validate Model Generalization

Ensure the model performs well on new, unseen data (test set).

Optimize Hyperparameters

Tune parameters to improve model performance.

Ensure Fairness & Robustness

Check if the model is biased or sensitive to small input changes.
