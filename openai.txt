import openai

openai.api_base = "http://localhost:8000/v1"  # vLLM's default OpenAI-compatible endpoint
openai.api_key = "EMPTY"

response = openai.ChatCompletion.create(
    model="opt-125m",  # Change to "codellama-7b-instruct-hf" for the other model
    messages=[{"role": "system", "content": "You are a helpful assistant."},
              {"role": "user", "content": "What is AI?"}]
)

print(response)

import asyncio

async def get_response(model_name, prompt):
    response = openai.ChatCompletion.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}]
    )
    return response

async def main():
    models = ["opt-125m", "codellama-7b-instruct-hf"]
    tasks = [get_response(model, "Explain machine learning.") for model in models]
    responses = await asyncio.gather(*tasks)
    for model, res in zip(models, responses):
        print(f"Model: {model}, Response: {res}")

asyncio.run(main())



import openai

openai.api_base = "http://localhost:8000/v1"
openai.api_key = "EMPTY"

response = openai.ChatCompletion.create(
    model="opt-125m",
    messages=[{"role": "user", "content": "Explain AI"}]
)

print(response)


======================================

from openai import OpenAI

# Point the client to your local vLLM server
client = OpenAI(
    base_url="http://localhost:8000/v1",  # vLLM's OpenAI-compatible API endpoint
    api_key="token-abc123"  # API key is required but can be arbitrary for vLLM
)

# Define your question
question = "Explain the concept of gravitational waves in simple terms."

# Generate a response
response = client.completions.create(
    model="facebook/opt-1.3b",  # The model you're using
    prompt=f"Question: {question}\nAnswer:",
    max_tokens=150,
    temperature=0.7,
    stop=["\n\n"]  # Stop generation at double newlines
)

# Print the answer
print("Question:", question)
print("Answer:", response.choices[0].text.strip())



from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="token-abc123")

questions = [
    "What is the capital of France?",
    "Explain quantum computing in simple terms.",
    "Who wrote 'To Kill a Mockingbird'?",
    "How does photosynthesis work?"
]

for question in questions:
    response = client.completions.create(
        model="facebook/opt-1.3b",
        prompt=f"Question: {question}\nAnswer:",
        max_tokens=100,
        temperature=0.3
    )
    print(f"Q: {question}")
    print(f"A: {response.choices[0].text.strip()}\n")


try:
    response = client.completions.create(
        model="facebook/opt-1.3b",
        prompt=prompt,
        max_tokens=100
    )
except Exception as e:
    print(f"Error processing question: {question}")
    print(f"Error: {str(e)}")
    continue
==================================

Chat completion:
response = client.chat.completions.create(
    model="facebook/opt-1.3b",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": question}
    ],
    max_tokens=150,
    temperature=0.7
)

print("Answer:", response.choices[0].message.content)



=========================
Conversation:
conversation = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Who invented the telephone?"},
    {"role": "assistant", "content": "Alexander Graham Bell is credited with inventing the telephone."},
    {"role": "user", "content": "What year was it invented?"}
]

response = client.chat.completions.create(
    model="facebook/opt-1.3b",
    messages=conversation,
    max_tokens=50
)

print(response.choices[0].message.content)

============================
Batch processing high through put:
batch_prompts = [f"Question: {q}\nAnswer:" for q in questions]

responses = client.completions.create(
    model="facebook/opt-1.3b",
    prompt=batch_prompts,
    max_tokens=100,
    temperature=0.3
)

for q, resp in zip(questions, responses.choices):
    print(f"Q: {q}")
    print(f"A: {resp.text.strip()}\n")



======================================================

import streamlit as st
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.entrypoints.openai import api_server
import threading
import openai
import uvicorn

# Configuration
MODEL_NAME = "facebook/opt-125m"  # Small model suitable for CPU
API_PORT = 8000

# --- vLLM Server Setup ---
def run_server():
    app = FastAPI()

    # Configure CORS
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Initialize the engine with CPU compatibility
    engine_args = AsyncEngineArgs(
        model=MODEL_NAME,
        tensor_parallel_size=1,
        gpu_memory_utilization=0.9,
        device="cpu",  # Force CPU usage
        dtype="float32",  # Use float32 for CPU compatibility
    )

    engine = AsyncLLMEngine.from_engine_args(engine_args)
    app = api_server.create_app(engine=engine)

    # Run without uvicorn (using uvicorn.run internally)
    config = uvicorn.Config(
        app,
        host="0.0.0.0",
        port=API_PORT,
        log_level="info",
        workers=1,
    )
    server = uvicorn.Server(config)
    server.run()

# --- Streamlit Interface ---
def streamlit_interface():
    # Configure OpenAI client to point to our vLLM server
    openai.api_base = f"http://localhost:{API_PORT}/v1"
    openai.api_key = "no-key-needed"

    st.title("vLLM Chat Interface (CPU)")
    st.write(f"Using model: {MODEL_NAME}")

    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display chat messages
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Accept user input
    if prompt := st.chat_input("What would you like to ask?"):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # Display user message
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Display assistant response
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            # Call vLLM server with streaming
            try:
                for response in openai.ChatCompletion.create(
                    model=MODEL_NAME,
                    messages=[{"role": m["role"], "content": m["content"]} 
                            for m in st.session_state.messages],
                    stream=True,
                    temperature=0.7,
                ):
                    full_response += response.choices[0].delta.get("content", "")
                    message_placeholder.markdown(full_response + "â–Œ")
                
                message_placeholder.markdown(full_response)
            except Exception as e:
                st.error(f"Error calling the model: {str(e)}")
                full_response = "Sorry, I encountered an error."
                message_placeholder.markdown(full_response)
        
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": full_response})

# --- Main Execution ---
if __name__ == "__main__":
    # Start the server in a separate thread
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    
    # Give the server a moment to start
    import time
    time.sleep(2)
    
    # Run the Streamlit interface
    streamlit_interface()
