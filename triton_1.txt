wget https://github.com/triton-inference-server/server/releases/download/v2.31.0/tritonserver2.31.0-jetpack5.1.tgz

Add triton to your PATH:
export PATH=$(pwd)/tritonserver/bin:$PATH

Step 2: Create a Triton Model Repository
mkdir -p triton_model_repository/codellama-7b-instruct/1


Copy the model files into the repository:

#Create a config.pbtxt file for the model
name: "codellama-7b-instruct"
platform: "vllm"
max_batch_size: 8
input [
  {
    name: "prompt"
    data_type: TYPE_STRING
    dims: [ -1 ]
  }
]
output [
  {
    name: "output"
    data_type: TYPE_STRING
    dims: [ -1 ]
  }
]


Step 3: Start Triton Inference Server
tritonserver --model-repository=$(pwd)/triton_model_repository


Install Triton Client
pip install tritonclient[http]

Send a Request
import tritonclient.http as httpclient
import numpy as np

# Initialize the client
client = httpclient.InferenceServerClient(url="localhost:8000")

# Prepare the input
inputs = [httpclient.InferInput("prompt", [1], "BYTES")]
inputs[0].set_data_from_numpy(np.array(["Write a Python function to calculate the factorial of a number."], dtype=object))

# Send the request
outputs = [httpclient.InferRequestedOutput("output")]
response = client.infer(model_name="codellama-7b-instruct", inputs=inputs, outputs=outputs)

# Get the output
result = response.as_numpy("output")
print(result)

6. Optimize for CPU
export OMP_NUM_THREADS=160


=+===============================
Alternative:
6. Alternative: Build Triton from Source
If the prebuilt binaries are not compatible with your system, you can build Triton Inference Server from source.

Step 1: Install Build Dependencies
Install required packages:


sudo yum install -y git cmake gcc-c++ make python3-devel
Install Python dependencies:


pip install numpy pybind11
Step 2: Build Triton
Clone the Triton Inference Server repository:


git clone https://github.com/triton-inference-server/server.git
cd server
Build Triton:


mkdir build && cd build
cmake -DTRITON_ENABLE_GPU=OFF ..
make -j$(nproc)
Install Triton:


make install
Add Triton to your PATH:


export PATH=$(pwd)/install/bin:$PATH
7. Test Triton
Start the Triton server:


tritonserver --model-repository=$(pwd)/triton_model_repository
Verify the server is running:

Check the logs for any errors.

Send a test request using the Triton HTTP or gRPC API.

