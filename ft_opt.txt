import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Initialize lemmatizer and stopwords
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Tokenize
    tokens = word_tokenize(text.lower())
    # Remove stopwords and non-alphabetic tokens
    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]
    return " ".join(tokens)

# Example usage
with open("your_text_file.txt", "r") as file:
    text_data = file.read()

preprocessed_text = preprocess_text(text_data)


#Prepare Dataset:
from datasets import Dataset

# Split text into chunks (e.g., sentences or paragraphs)
text_chunks = preprocessed_text.split("\n")  # Adjust based on your data

# Create a dataset
dataset = Dataset.from_dict({"text": text_chunks})


#Load OPT and tokenizer:
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "facebook/opt-125m"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

#Tokenize dataset:
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True)



#Apply peft:
from peft import get_peft_model, LoraConfig, TaskType

# Define LoRA configuration
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,  # Task type for causal language modeling
    inference_mode=False,
    r=8,  # Rank of the low-rank matrices
    lora_alpha=32,  # Scaling factor
    lora_dropout=0.1,  # Dropout rate
)

# Apply PEFT to the model
peft_model = get_peft_model(model, peft_config)


#Fine training:
from transformers import Trainer, TrainingArguments

# Define training arguments
training_args = TrainingArguments(
    output_dir="./opt-125m-finetuned",
    per_device_train_batch_size=4,
    num_train_epochs=3,
    logging_dir="./logs",
    logging_steps=10,
    save_steps=500,
    save_total_limit=2,
)

# Initialize Trainer
trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=tokenized_dataset,
)

# Fine-tune the model
trainer.train()

from peft import PeftModel

# Assuming `model` is your fine-tuned PeftModel
# and `original_model` is the base model (e.g., from transformers)

# Save the adapter weights and config
model.save_pretrained("path/to/save/adapter")
#Save:
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PeftConfig

# Step 1: Load the base model and tokenizer
base_model_name = "original_model_name_or_path"
base_model = AutoModelForCausalLM.from_pretrained(base_model_name)
tokenizer = AutoTokenizer.from_pretrained(base_model_name)

# Step 2: Load the fine-tuned adapter
peft_model_id = "path/to/save/adapter"
model = PeftModel.from_pretrained(base_model, peft_model_id)

# Step 3: Merge the adapter with the base model (optional)
merged_model = model.merge_and_unload()
merged_model.save_pretrained("path/to/save/merged_model")
tokenizer.save_pretrained("path/to/save/merged_model")

# Step 4: Use the merged model with vLLM
from vllm import LLM

llm = LLM(model="path/to/save/merged_model")

===========================

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load the base model
base_model_name = "base_model_name_or_path"
base_model = AutoModelForCausalLM.from_pretrained(base_model_name)

# Load the fine-tuned tokenizer
tokenizer = AutoTokenizer.from_pretrained("path/to/save/tokenizer")  # Fine-tuned tokenizer
print(f"Tokenizer vocab size: {len(tokenizer)}")  # Should be 50272

# Resize the base model's embedding layer to match the new tokenizer size
base_model.resize_token_embeddings(len(tokenizer))

# Load the fine-tuned adapter
peft_model_id = "path/to/save/adapter"
model = PeftModel.from_pretrained(base_model, peft_model_id)


# Check the model's embedding layer size
embedding_size = model.base_model.model.get_input_embeddings().weight.shape[0]
print(f"Model embedding size: {embedding_size}")  # Should be 50272



# Save the resized base model with the adapter
model.save_pretrained("path/to/save/resized_model")
tokenizer.save_pretrained("path/to/save/resized_model")




# Merge the adapter with the base model
merged_model = model.merge_and_unload()

# Save the merged model
merged_model.save_pretrained("path/to/save/merged_model")
tokenizer.save_pretrained("path/to/save/merged_model")



---------------------
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Step 1: Load the base model
base_model_name = "base_model_name_or_path"
base_model = AutoModelForCausalLM.from_pretrained(base_model_name)

# Step 2: Load the fine-tuned tokenizer
tokenizer = AutoTokenizer.from_pretrained("path/to/save/tokenizer")
print(f"Tokenizer vocab size: {len(tokenizer)}")  # Should be 50272

# Step 3: Resize the base model's embedding layer
base_model.resize_token_embeddings(len(tokenizer))

# Step 4: Load the fine-tuned adapter
peft_model_id = "path/to/save/adapter"
model = PeftModel.from_pretrained(base_model, peft_model_id)

# Step 5: Save the resized model and tokenizer
model.save_pretrained("path/to/save/resized_model")
tokenizer.save_pretrained("path/to/save/resized_model")

# Step 6: Merge the adapter (optional)
merged_model = model.merge_and_unload()
merged_model.save_pretrained("path/to/save/merged_model")
tokenizer.save_pretrained("path/to/save/merged_model")

# Step 7: Use with vLLM
from vllm import LLM
llm = LLM(model="path/to/save/merged_model")
